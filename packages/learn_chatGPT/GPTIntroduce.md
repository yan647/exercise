## 大模型学习笔记

### gpt相关参数
1. **Temperature**：用来控制生成文本的随机性。`temperature` 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，
大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。在实际应用方面，对于质量保障（QA）等任务，
可以设置更低的`temperature`值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，可以适当调高`temperature`参数值。
在具体操作中，模型会计算每个可能的下一个词的概率，然后根据这些概率来随机选择下一个词。Temperature参数会影响这个概率分布。
   1. 当Temperature接近0时，模型会更倾向于选择概率最高的词，这会导致生成的文本比较确定性，但可能缺乏多样性。
   2. 当Temperature接近1时，模型会更公平地考虑所有的词，这会导致生成的文本更具随机性和多样性，但可能会牺牲一些连贯性。

    所以，Temperature参数可以被看作是控制生成文本"创新性"和"确定性"之间平衡的一个工具。通过调整这个参数，我们可以得到不同风格的生成文本。

2. **Top_p（也被称为nucleus sampling或者top-p sampling）**：是一种采样技术。可以用来控制模型返回结果的真实性。如果你需要准确和真实的答案，就把参数值调低。如果你想要更多样化的答案，就把参数值调高一些。

    #### temperature与Top_p的区别
    Temperature和Top_p都是控制文本生成过程中随机性的参数，但它们的工作方式有所不同。
    
    Temperature（温度）参数主要影响模型生成每个可能词的概率分布。Temperature越高，生成的文本越随机和多样；Temperature越低，生成的文本越确定，但可能缺乏多样性。
    
    而Top_p（也被称为nucleus sampling或者top-p sampling）则是在每一步生成新的词时，模型会计算所有可能词的概率分布。
    然后，它会选择一个概率阈值p，只考虑概率之和大于p的最小词集，然后从这个词集中随机选择一个词作为新的词。
    
    总的来说，Temperature和Top_p都可以控制生成文本的随机性和多样性，但它们的具体方法和侧重点有所不同。
    Temperature更侧重于调整整体的概率分布，而Top_p则更侧重于限制可选词的范围。
    
    #### 什么是核采样
    **核采样（Nucleus Sampling）** 工作原理是这样的：在每一步生成新的词时，模型会计算所有可能词的概率分布。然后，它会选择一个概率阈值p（0<p<1），并只考虑概率之和大于p的最小词集。然后，从这个词集中随机选择一个词作为新的词。这个词集就是所谓的"核"。
    
    这种方法的优点是，它可以在生成多样性和连贯性之间找到一个好的平衡。通过调整阈值p，我们可以控制生成文本的多样性。如果p较大，那么生成的文本会更多样，但可能会较少连贯。如果p较小，那么生成的文本会更连贯，但可能会较少多样。

3. Top_k：这是一种叫做"Top-k sampling"的方法。在每一步生成新的词时，模型会计算所有可能词的概率分布。然后，它只考虑概率最高的k个词，然后从这k个词中随机选择一个词作为新的词。这种方法可以防止模型生成一些概率极低的词，从而提高生成文本的质量。

4. Length_penalty：这是一种用于控制生成文本长度的参数。在一些应用中，我们可能希望生成的文本长度能够接近一个特定的值。通过调整Length_penalty参数，我们可以鼓励模型生成更长或者更短的文本。

5. Repetition_penalty：这是一种用于防止生成重复文本的参数。在一些情况下，模型可能会反复生成同样的词或者短语。通过调整Repetition_penalty参数，我们可以降低这种重复的可能性。

### 提示技术

1. **零样本提示（zero-shot prompting）**：即用户不提供任务结果相关的示范，直接提示语言模型给出任务相关的回答。某些大型语言模式有能力实现零样本提示，但这也取决于任务的复杂度和已有的知识范围。

2. **小样本提示（Few-shot Prompting）**：目前业界普遍使用的还是更高效的_小样本提示（Few-shot Prompting）_范式，即用户提供少量的提示范例，如任务说明等。语言模型可以基于一些说明了解和学习某些任务，而小样本提示正好可以赋能上下文学习能力。

3. **链式思考（Chain-of-Thought Prompting，CoT）提示**：是一种在GPT模型中引导模型生成特定内容的策略。这种策略的基本思想是，通过将多个提示串联起来，模拟人类的思考过程，从而引导模型生成更符合人类思维方式的内容。具体来说，我们可以首先给出一个初始的提示，让模型生成一些内容。然后，我们可以根据模型生成的内容，提出新的提示，让模型继续生成。这个过程可以重复多次，形成一个链式的思考过程。链式思考（CoT）提示通过中间推理步骤实现了复杂的推理能力。可以将其与少样本提示相结合，以获得更好的结果，以便在回答之前进行推理的更复杂的任务。

4. **自我一致性**：旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本CoT采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务中的性能。

    在使用GPT模型时，我们可以通过以下方式引导模型保持自我一致性：
    
    1. 提供详细的上下文：在给模型提供输入时，我们应该尽可能提供详细的上下文信息。这样，模型就可以根据这些信息生成与之一致的内容。例如，如果我们想要模型写一个关于勇敢骑士的故事，我们可以在输入中明确指出这个骑士的性格特点和历史背景。
    
    2. 使用明确的指示：在给模型提供输入时，我们可以使用明确的指示来引导模型生成我们想要的内容。例如，我们可以明确指出我们希望模型描述骑士如何勇敢地战斗，而不是描述他的其他方面。
    
    3. 进行多轮的交互：在使用GPT模型时，我们可以进行多轮的交互，以确保模型生成的内容与我们的期望保持一致。例如，如果模型在第一轮生成的内容与我们的期望不符，我们可以在第二轮中提供更明确的指示，引导模型生成正确的内容。

5. **生成知识提示**: LLM在执行需要更多关于世界的知识的任务时有很大的局限性。我们可以使用生成知识以作为提示的一部分来改进。

6. **思维树（Tree of Thoughts，ToT）框架**：该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LM 能够自己对严谨推理过程的中间思维进行评估。LM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。（开始用上算法了，比如BFS、DFS）

7. **检索增强生成 (RAG)**：用以完成知识密集型的任务。通过基于语言模型构建一个系统，访问外部知识源来做到。RAG 把一个信息检索组件和文本生成模型结合在一起。RAG 可以微调，其内部知识的修改方式很高效，不需要对整个模型进行重新训练。RAG 会接受输入并检索出一组相关/支撑的文档，并给出文档的来源（例如维基百科）。这些文档作为上下文和输入的原始提示词组合，送给文本生成器得到最终的输出。这样 RAG 更加适应事实会随时间变化的情况。这非常有用，因为 LLM 的参数化知识是静态的。RAG 让语言模型不用重新训练就能够获取最新的信息，基于检索生成产生可靠的输出。

8. ART（Automatic Reasoning and Tool-use）的工作原理如下：

   1. 接到一个新任务的时候，从任务库中选择多步推理和使用工具的示范。
   2. 在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成。
   
   ART 引导模型总结示范，将新任务进行拆分并在恰当的地方使用工具。ART 采用的是零样本形式。ART 还可以手动扩展，只要简单地更新任务和工具库就可以修正推理步骤中的错误或是添加新的工具。


### 参考
1. [Google生成式AI科普教程](https://www.cloudskillsboost.google/journeys/118) 
2. [prompt工程指南](https://www.promptingguide.ai/zh)  
3. https://km.sankuai.com/page/1607779952
4. https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api
5. https://learnprompting.org/zh-Hans/docs/intro
